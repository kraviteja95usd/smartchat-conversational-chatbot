{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8ef13c-e5bf-4899-9ceb-71bfe1105bab",
   "metadata": {},
   "source": [
    "**Authors:**\n",
    "\n",
    "- Ravi Teja Kothuru (Primary)\n",
    "- Soumi Ray\n",
    "- Anwesha Sarangi\n",
    "\n",
    "**Title of the Project:** SmartChat: A Context-Aware Conversational Agent\n",
    "\n",
    "**Description of the Project:** Develop a chatbot that can effectively adapt to context and topic shifts in a conversation, leveraging the Stanford Question Answering Dataset to provide informed and relevant responses, and thereby increasing user satisfaction and engagement.\n",
    "\n",
    "**Objectives of the Project:** Create a user-friendly web or app interface that enables users to have natural and coherent conversations with the chatbot, with high satisfaction rating.\n",
    "\n",
    "**Name of the Dataset:** Stanford Question Answering Dataset\n",
    "\n",
    "**Description of the Dataset:** The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset consisting of questions posed by crowdworkers on a set of Wikipedia articles. The answer to every question is a segment of text, or span, from the corresponding reading passage. There are 100,000+ question-answer pairs on 500+ articles. More information can be found at: https://rajpurkar.github.io/SQuAD-explorer/\n",
    "\n",
    "**Dataset Source:**\n",
    "\n",
    "Kaggle (https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset)\n",
    "\n",
    "***Number of Variables in Dataset:*** There are 2 variables in this dataset\n",
    "\n",
    "- data\n",
    "- version\n",
    "\n",
    "Each of these have other variables such as:\n",
    "\n",
    "- ***context:*** A lengthy paragraph that has some information.\n",
    "- ***question:*** A question based on the context.\n",
    "- ***answer:*** An answer to the context from the context.\n",
    "- ***ans_start:*** The index value of context where the answer to the question is started.\n",
    "- ***ans_end:*** The index value of context where the answer to the question is ended.\n",
    "\n",
    "***Size of the Dataset:*** The dataset has 2 JSON files. One is for training and the other is for validation\n",
    "\n",
    "- Training Dataset's filename is train-v1.1.json and it size is 30.3 MB.\n",
    "- Validation Dataset's filename is dev-v1.1.json and it size is 4.9 MB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c6916-3c72-4c24-9fa0-c62225e142a7",
   "metadata": {},
   "source": [
    "# Install/Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7b7e69-20fc-4bc5-9de7-9608e35ba9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: torch in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (2.5.0)\n",
      "Requirement already satisfied: peft in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: transformers in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (4.45.2)\n",
      "Requirement already satisfied: evaluate in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: safetensors in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (0.4.5)\n",
      "Requirement already satisfied: numpy in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (2.1.2)\n",
      "Requirement already satisfied: pandas in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (1.5.2)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (0.26.0)\n",
      "Requirement already satisfied: packaging in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from peft) (1.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.15.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ravi/Documents/my_projects/chatbot-using-gpt2-medium/jupyter_env/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m733.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=209dd16b01ff9354d790eb51c82ce75b1a14ab561cf6c3bf643454c8b1c44b67\n",
      "  Stored in directory: /home/ravi/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: click, absl-py, nltk, rouge-score\n",
      "Successfully installed absl-py-2.1.0 click-8.1.7 nltk-3.9.1 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries: datasets for dataset handling, torch for PyTorch framework,\n",
    "# peft for parameter-efficient fine-tuning, transformers for NLP model handling,\n",
    "# evaluate for model evaluation, and safetensors for safe tensor handling.\n",
    "!pip install datasets torch peft transformers evaluate safetensors numpy pandas matplotlib scikit-learn nltk rouge-score\n",
    "\n",
    "# Import the PyTorch library for building and training neural networks.\n",
    "import torch\n",
    "\n",
    "# Call the garbage collector to free up memory; useful for managing memory during model training.\n",
    "import gc\n",
    "\n",
    "# Import the necessary classes from the transformers library:\n",
    "# - AutoTokenizer: Automatically selects the correct tokenizer for a given model.\n",
    "# - AutoModelForCausalLM: Automatically loads the correct causal language model.\n",
    "# - AutoConfig: Loads the configuration of the model, useful for understanding model architecture.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "# Libraries essential for setting up and managing the training process of machine learning models.\n",
    "from transformers import TrainingArguments,Trainer\n",
    "\n",
    "# Import functions from safetensors library to handle safe tensor loading and saving.\n",
    "from safetensors.torch import load_model, save_model\n",
    "\n",
    "# Import the load_dataset function from the datasets library to easily load datasets for training or evaluation.\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import the evaluate library to perform evaluations on model predictions.\n",
    "import evaluate\n",
    "\n",
    "# Import PeftModel and PeftConfig from the peft library for efficient fine-tuning of models.\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Importing necessary classes for configuring and using Low-Rank Adaptation in model tuning.\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# Import the os module to interact with the operating system, such as file paths and directories.\n",
    "import os\n",
    "\n",
    "# Importing the AutoPeftModelForCausalLM class from the peft library for fine-tuning causal language models.\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9f40f-2a7e-43c7-aa29-73d4eef660fb",
   "metadata": {},
   "source": [
    "# Decide which architecture to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873525c-5302-43fc-8743-8f1bb69361eb",
   "metadata": {},
   "source": [
    "## Comparison of the architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbfc04f-ca2e-4e7a-9198-ec9896e8d13c",
   "metadata": {},
   "source": [
    "To develop chatbots, we have different architectures. \n",
    "Let us better understand about them before deciding which one to use.\n",
    "\n",
    "# Differences Between Seq2Seq, Transformers, GPT, and GPT-2 (Small, Medium & Large)\n",
    "\n",
    "| Feature       | Seq2Seq                                           | Transformers                                    | GPT                                               | GPT-2 Small, Medium & Large                       |\n",
    "|---------------|--------------------------------------------------|------------------------------------------------|--------------------------------------------------|--------------------------------------------------|\n",
    "| **Definition**| A model that transforms an input sequence into an output sequence using an encoder and decoder. | A deep learning architecture using self-attention mechanisms to process input sequences. | A specific Transformer model designed for generating text by predicting the next word in a sequence. | Variants of the GPT model with different sizes and capacities for generating text. |\n",
    "| **Usage**     | Tasks where input and output are sequences, like translation and summarization. | A wide range of NLP tasks, including translation and summarization. | Primarily used for text generation tasks like chatbots and text completion. | Used for similar text generation tasks, with larger models generally providing better performance and coherence. |\n",
    "| **Information**| Consists of an encoder that processes the input and a decoder that generates the output. | Composed of an encoder and decoder stack, using self-attention to capture relationships between words. | Utilizes only the decoder part of the Transformer, focusing on unidirectional text generation. | Small has fewer parameters, while medium and large have progressively more, enhancing their ability to understand and generate text. |\n",
    "| **Strengths** | Effective for varying output lengths; good at capturing context. | Can process sequences in parallel; captures long-range dependencies well. | Excellent at generating coherent and contextually relevant text; adapts to various topics. | Larger models (medium and large) can generate more sophisticated and nuanced text compared to the small model. |\n",
    "| **Limitations**| Struggles with long sequences due to fixed-length context vectors; may not capture long-range dependencies well. | Requires substantial data and computational power; complexity can make fine-tuning harder. | May generate repetitive or nonsensical outputs; unidirectional nature limits contextual understanding compared to bidirectional models. | Small may struggle with complexity in tasks, while larger models require more computational resources and memory. |\n",
    "| **Applications**| Machine translation, text summarization, conversational agents. | Machine translation, text generation, sentiment analysis. | Chatbots, text completion, creative writing assistance. | Similar applications as GPT, with larger models often preferred for more demanding tasks. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbddf21-f9df-4bc0-9c31-9020183f61e2",
   "metadata": {},
   "source": [
    "## Final Decision of the Architecture to use for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb1570-26d4-4f97-b384-bfd2a0cbd76c",
   "metadata": {},
   "source": [
    "I have decided to use ***GPT-2 Medium*** sized model because of the following reasons:\n",
    "\n",
    "- Excellent at generating coherent and contextually relevant texts.\n",
    "- Considering the time and computational resources constraints, we need a model that has optimal size and generates relevant text properly. Therefore, GPT2-medium sized architectural model would be sufficient.\n",
    "- Provides a detailed understanding of the question asked based on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d52d7-3677-4d2e-a2cb-ecd10e0d7edf",
   "metadata": {},
   "source": [
    "# Load the GPT2-Medium Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1520f88-b61a-470f-a5c6-3ed8b915f746",
   "metadata": {},
   "source": [
    "1. **Load a Text Processing Tool:** The code is using a specific tool, which is nothing but `openai-community/gpt2-medium`, designed to convert text into a format that a model can understand. This tool is associated with a specific version of a language model.\n",
    "\n",
    "2. **Set Padding Token:** The code assigns a special symbol to represent empty spaces or padding in the text. This symbol helps the model understand when there is no actual content.\n",
    "\n",
    "3. **Assign Padding Token ID:** The code also sets an identifier for the padding symbol. This identifier is a number that the model uses to recognize the padding symbol in the processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95b52a6-e23b-405f-bb2f-0834f14ca9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60d91ccc956478f96f4700c629fa45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15c4dee4e5c4878a1e3f150e0d55d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93561975e42c4a789f09bd1ae716e0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39b049d7d834a719b1a8b3c0dd6819a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645622feb5124839a3520a7dd3ae4fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"openai-community/gpt2-medium\"\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01075f83-30c5-40bf-bdf2-0f38fd7ea92e",
   "metadata": {},
   "source": [
    "# Load the `openai-community/gpt2-medium` base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb049a-a7a9-4af2-a2f1-97b63f2c6ad3",
   "metadata": {},
   "source": [
    "**Automatic configuration:**\n",
    "\n",
    "1. **Objective**: The goal is to manage the configuration settings for different models in a uniform manner through automation. It makes loading model configurations easier.\n",
    "\n",
    "2. **Management of Configuration**: It oversees factors like the quantity of layers, hidden size, and vocabulary size.\n",
    "\n",
    "3. **Simplicity of Use**: `AutoConfig` allows users to avoid manually setting multiple parameters, as they can easily load them by providing the model name. This decreases the possibility of mistakes and simplifies transitioning between various models.\n",
    "\n",
    "4. **Compatibility**: The set-up is customized to match the particular model design, guaranteeing all adjustments are suitable for the model in use.\n",
    "\n",
    "**AutoModelForCausalLM:** is used for generating language models that predict the next word in a sequence based on the input sequence.\n",
    "\n",
    "1. **Goal**: This class is made for designing models that produce text in a sequential way. This means the model uses the previous words in a sequence to predict the next word.\n",
    "\n",
    "2. **Pre-trained Models**: Users can import pre-trained causal language models that have already been trained on extensive datasets. This saves time and computational resources by eliminating the need for the user to train the model from the beginning.\n",
    "\n",
    "3. **Ability to Generate**: The model's capabilities include text completion, dialogue generation, and other tasks that require producing coherent sequences of text.\n",
    "\n",
    "4. **Adaptable and Expandable**: This class is compatible with various iterations of generative models, simplifying the utilization of diverse architectures (such as GPT-2, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e377569-eb97-4ac2-bae8-f5cb51cf8c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70698b1353e9451fbf85f90c6ac48bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a484f5cd215c41c5aa1353792a3beaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c25f4c5de14619af238f326c1faba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7dca57335dd413ab4d1fc19bb35f1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b202cabc0d6143ab93e5d92b883722b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a2ec6eefd94c2fa117a629de897099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12c8ccd981e40809b2e5782a23f613c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load  model\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"openai-community/gpt2-medium\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"openai-community/gpt2-medium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f2fd7-078d-4473-b413-9cd061536757",
   "metadata": {},
   "source": [
    "# Load SQuAD dataset from HuggingFace datasets library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ec0c9-6269-41f0-be34-fd349a98e863",
   "metadata": {},
   "source": [
    "- Please be informed - this SQuAD (Stanford Question Answering) dataset is available in HuggingFace's `datasets` library itself. Hence, there is no need of downloading the dataset files to the local and loading them.\n",
    "\n",
    "- Since we are working on SQuAD v1.1, `squad` should be sufficient. The `load_dataset` function from `datasets` library helps in loading the required dataset.\n",
    "\n",
    "- As part of memory management, I have deleted the actual loaded dataset variables post splitting them into `train_ds` and `eval_ds`.\n",
    "\n",
    "- Cleanup unused memory that is no longer needed by the program, using `gc.collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68ea2a71-3b7a-4632-a753-d274fbb9db9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1962"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # load dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "vali_ds = dataset['validation']\n",
    "spilt_ds = dataset['train'].train_test_split(test_size=0.2)\n",
    "train_ds = spilt_ds['train'].shuffle(seed=42)\n",
    "eval_ds = spilt_ds['test'].shuffle(seed=42)\n",
    "\n",
    "# clear original dataset\n",
    "del dataset\n",
    "del spilt_ds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd428d-2c55-4e62-adde-48b5b18cf44c",
   "metadata": {},
   "source": [
    "# Verify the existence of CPU/GPU and confirm what is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cc5e239-6eff-4f71-b4e7-f920d843341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using the device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available. If yes, use it. Else, use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Currently using the device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702b346-1591-477e-bfbd-02c49a3df0d6",
   "metadata": {},
   "source": [
    "# Preprocess the data as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3904d03-2ae6-453f-a1b9-16ba46ba4b68",
   "metadata": {},
   "source": [
    "Before proceeding with the data preprocessing and making it ready for training, let us first understand the meaning of LoRA and PEFT.\n",
    "\n",
    "# Comparison of LoRA and PEFT\n",
    "\n",
    "| Aspect               | LoRA (Low-Rank Adaptation)                                    | PEFT (Parameter-Efficient Fine-Tuning)                      |\n",
    "|----------------------|--------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Definition**       | A technique that adapts pre-trained models by introducing low-rank matrices, allowing for efficient model tuning. | A broader framework for fine-tuning models with minimal parameter updates, enhancing efficiency and speed. |\n",
    "| **Purpose**          | To reduce the number of parameters that need to be updated during model training while maintaining performance. | To fine-tune large models with a focus on reducing computational costs and resource usage. |\n",
    "| **Examples**         | Used in language models like GPT-2 to enhance performance with fewer resources. | Applied in various architectures (e.g., BERT, T5) for tasks such as text classification, summarization, and more. |\n",
    "| **Usages**           | Primarily used for fine-tuning large pre-trained models in specific tasks with limited data or computational resources. | Utilized in a wide range of applications, including natural language processing, computer vision, and more, where full fine-tuning is impractical. |\n",
    "| **Methodology**      | Involves modifying only certain layers or components of a model, keeping the majority of the parameters unchanged. | Can include methods like LoRA, prompt tuning, and adapter layers to achieve efficient fine-tuning. |\n",
    "| **Benefits**         | Reduces memory and computational overhead, making it feasible to use large models in resource-constrained environments. | Allows for faster training times and lower resource consumption while still achieving high performance. |\n",
    "| **Limitations**      | May not achieve the same level of performance as full fine-tuning in some cases. | The effectiveness can vary depending on the specific task and model architecture used. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a6abb-4aaf-42b5-90de-7acf5d05762b",
   "metadata": {},
   "source": [
    "## Make use of LORA and PEFT\n",
    "\n",
    "1. **Configuration Creation**: The code is creating a new configuration for a specific type of model tuning.\n",
    "\n",
    "2. **Low-Rank Adaptation (LoRA)**: The technique being used is called Low-Rank Adaptation, which helps to adapt pre-trained models efficiently.\n",
    "\n",
    "    3. **Rank Parameter**: A parameter is set that controls the rank of the adaptation, allowing the model to learn from a lower-dimensional representation.\n",
    "\n",
    "    4. **Scaling Factor**: Another parameter defines a scaling factor that adjusts how much influence the adaptation has during the training process.\n",
    "\n",
    "    5. **Dropout Rate**: A value is specified for dropout, which is a method used to prevent overfitting by randomly ignoring some neurons during training.\n",
    "\n",
    "    6. **Input/Output Configuration**: The setup includes an option for how inputs and outputs are handled in the adaptation process.\n",
    "\n",
    "    7. **Bias Handling**: The configuration specifies whether to use bias in the model adaptation, in this case, it is set not to include bias.\n",
    "\n",
    "    8. **Task Type Specification**: Finally, the code indicates that the model is being adapted for a specific task type, which is generating text based on previous inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b70d5a8d-1308-4e96-ae1a-3b901467c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    fan_in_fan_out=True,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf426360-cee8-4c68-9490-6d5177359f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786,432 || all params: 355,609,600 || trainable%: 0.2212\n",
      "Deleted unuesed memory from GPU\n"
     ]
    }
   ],
   "source": [
    "# Apply the LoRA's config into PEFT model\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "# Send the LoRA model object to the device (CPU/GPU) for further processing and training\n",
    "lora_model.to(device)\n",
    "\n",
    "# Print the count of trainable params count and %\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Delete unused memory/cache from GPU\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Deleted unuesed memory from GPU\")\n",
    "\n",
    "#define preprocess function\n",
    "def preprocess_function(examples):\n",
    "    # Format the required input columns from the dataset into a list\n",
    "    inputs = [f\"Context: {c}\\nQuestion: {q}\\nAnswer:\" for q, c in zip(examples['question'], examples['context'])]\n",
    "    \n",
    "    # Apply tokenization to inputs\n",
    "    model_inputs = tokenizer(inputs,padding=\"max_length\", truncation=True, max_length=256,return_tensors='pt')\n",
    "    \n",
    "    # Format the required target columns from the dataset into a list\n",
    "    targets = [','.join(a['text']) if len(a['text']) > 0 else '' for a in examples['answers']]\n",
    "\n",
    "    # Apply tokenization to targets\n",
    "    labels = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels['input_ids']\n",
    "    model_inputs[\"labels_mask\"] = labels['attention_mask']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Preprocess train information\n",
    "tok_train_ds = train_ds.map(preprocess_function, batched=True)\n",
    "tok_train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\",\"labels\",\"labels_mask\"])\n",
    "\n",
    "# Preprocess evaluation information\n",
    "tok_eval_ds = eval_ds.map(preprocess_function, batched=True)\n",
    "tok_eval_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\",\"labels\",\"labels_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8bd370-2567-4482-ab61-dee081dabe0c",
   "metadata": {},
   "source": [
    "# Train the model, save the result and set for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a2eea-6332-46a2-8ddf-3ec245a31ef5",
   "metadata": {},
   "source": [
    "Let us understand the following code to train the model.\n",
    "\n",
    "1. **Importing Libraries**:\n",
    "   - The code starts by importing `TrainingArguments` and `Trainer` from the `transformers` library, which are essential for setting up and managing the training process of machine learning models.\n",
    "\n",
    "2. **Setting Training Arguments**:\n",
    "   - A `TrainingArguments` object named `training_args` is created, where various parameters related to training are specified:\n",
    "   - `per_device_train_batch_size=32`: This sets the number of training samples to be processed at one time (batch size) to 32 for each device (like CPU or GPU).\n",
    "   - `per_device_eval_batch_size=32`: Similarly, this sets the evaluation batch size to 32.\n",
    "   - `output_dir=\"./results\"`: This specifies the directory where the training results (like model checkpoints) will be saved, here it's a folder named \"results\".\n",
    "   - `learning_rate=2e-4`: This sets the learning rate, which controls how much to change the model's weights during training; in this case, it is set to 0.0002.\n",
    "   - `weight_decay=0.01`: This adds a penalty to the model weights to prevent overfitting, with a decay factor of 0.01.\n",
    "   - `evaluation_strategy=\"epoch\"`: This indicates that the model will be evaluated at the end of each training epoch.\n",
    "   - `save_strategy=\"epoch\"`: This means the model will be saved at the end of each epoch as well.\n",
    "   - `load_best_model_at_end=True`: After training, the best model (based on evaluation metrics) will be loaded automatically.\n",
    "   - `num_train_epochs=3`: This sets the total number of times the model will go through the entire training dataset, which is 3 epochs.\n",
    "\n",
    "3. **Creating the Trainer**:\n",
    "   - A `Trainer` object named `trainer` is instantiated, which will handle the training and evaluation of the model. The following parameters are passed to it:\n",
    "   - `model=lora_model`: This specifies the model to be trained, which is referenced by the variable `lora_model`.\n",
    "   - `args=training_args`: This passes the training arguments that were defined earlier.\n",
    "   - `tokenizer=tokenizer`: This provides the tokenizer that will be used to preprocess the text data for the model.\n",
    "   - `train_dataset=tok_train_ds`: This sets the training dataset, which is referenced by the variable `tok_train_ds`.\n",
    "   - `eval_dataset=tok_eval_ds`: This sets the evaluation dataset, referenced by the variable `tok_eval_ds`.\n",
    "\n",
    "In summary, this code is preparing everything necessary to train a machine learning model using specified settings and datasets, while also ensuring that evaluation and saving of the best model are handled automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce942740-d03d-408b-a353-df7ceea82daa",
   "metadata": {},
   "source": [
    "## Interpretation post training\n",
    "\n",
    "- We noticed that post training, the evaluation loss is `0.1442018747329712` which means the model got trained very well.\n",
    "- Due to time and computational resources constraints, we have used a less powerful GPU. As a result, we had to use only 3 epochs and it still took 2 hours 26 minutes to complete the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23bd7255-5b47-4a4e-be84-85c4d231dcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6570' max='6570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6570/6570 2:26:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.147700</td>\n",
       "      <td>0.145340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.144450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.144202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='548' max='548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [548/548 03:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1442018747329712, 'eval_runtime': 223.1288, 'eval_samples_per_second': 78.52, 'eval_steps_per_second': 2.456, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Prepare for training the gpt2-medium model\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(model=lora_model,\n",
    "                  args=training_args,\n",
    "                  tokenizer=tokenizer,\n",
    "                  train_dataset=tok_train_ds,\n",
    "                  eval_dataset=tok_eval_ds\n",
    "                )\n",
    "\n",
    "# save the trained model\n",
    "lora_model.save_pretrained(\"gpt2-medium-lora\")\n",
    "\n",
    "# train the model\n",
    "trainer.train()\n",
    "\n",
    "# eval the traiend model\n",
    "evaluation_result = trainer.evaluate()\n",
    "print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151ee82-f70b-428a-a251-fe4434bbf534",
   "metadata": {},
   "source": [
    "# Validate the trained 'gpt2-medium' model using Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b931f49a-cf7e-4c52-a029-4546631a45ae",
   "metadata": {},
   "source": [
    "## Load the validation dataset\n",
    "\n",
    "- Here, we have directly loaded the validation dataset from HuggingFace's `datasets` library.\n",
    "- As there are more than 10000 records we initially loaded only the first 5 records of the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa1503f-fc2a-4492-8aef-5d3aef6a18ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77222132ec1441d59f2fb5e3f0830112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d077c10096f6474fa72c7e10fd727acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01458afff67493dbe24d97333bf830d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7281f4a11a4a8c962a0e840e7bff59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387e78d7b8fe415d898e124ba66a206c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")\n",
    "validation_ds = dataset['validation'].select(range(5))\n",
    "validation_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc008b4e-5e6e-4f37-aa14-4dbe7e1c85b3",
   "metadata": {},
   "source": [
    "# Function to format the input prompt and tokenize it\n",
    "\n",
    "- `prompts` list contains formatted strings for each question and its corresponding context.\n",
    "- For each pair of question (`q`) and context (`c`) in `examples`, the function creates a string that starts with `\"Context: \"`, followed by the context text, then `\"Question: \"`, followed by the question text, and finally ends with `\"<|start_answer|>\"`.\n",
    "- After creating the list of formatted strings, the `tokenizer` function is called. This function converts the list of prompts into numerical representations that the model can understand.\n",
    "\n",
    "**Parameters of Tokenization**:\n",
    "   - `padding=\"max_length\"`: This ensures that all the tokenized prompts are padded to the maximum length specified by `max_length`.\n",
    "   - `truncation=True`: This allows any prompts that exceed the `max_length` to be shortened.\n",
    "   - `max_length=256`: This sets the maximum length for each tokenized prompt to 256 tokens.\n",
    "   - `return_tensors='pt'`: This specifies that the output should be returned as PyTorch tensors, which can be used for model input.\n",
    "\n",
    "- Finally, the function returns the tokenized prompts, which are now in a format suitable for input into a model.\n",
    "\n",
    "Look at the following code for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd80fcdd-bae4-45c0-b89f-bfb7caa55fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompts(examples):\n",
    "    prompts = [f\"Context: {c}\\nQuestion: {q}\\n <|start_answer|>\" for q, c in zip(examples['question'], examples['context'])]\n",
    "    return tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175c02f-328a-4dfe-958f-a4b208cfb6da",
   "metadata": {},
   "source": [
    "- The function named `extract_answer`, which takes one parameter called `generated_text`. This parameter is expected to be a string containing some text.\n",
    "- `start_token` is defined with the value `\"<|start_answer|>\"`. This token is used to identify where the actual answer starts in the generated text.\n",
    "- The function searches for the `start_token` in the `generated_text` and finds its position using the `find` method. \n",
    "- It then adds the length of `start_token` to this position to determine the starting index (`start_idx`) of the actual answer text.\n",
    "- The code checks if `start_idx` is not equal to -1, which means the `start_token` was found in the text. \n",
    "- If the token is found, it extracts the answer text starting from `start_idx` and removes any leading or trailing whitespace using the `strip` method.\n",
    "\n",
    "**Purpose**:\n",
    "- The main purpose of this function is to isolate and return the answer portion of the `generated_text`, which is located after a specific start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6841df9-694d-4268-ba4a-1854ed7ca911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(generated_text):\n",
    "    start_token = \"<|start_answer|>\"\n",
    "    \n",
    "    # Extract the text between the special tokens\n",
    "    start_idx = generated_text.find(start_token) + len(start_token)\n",
    "    \n",
    "    if start_idx != -1:\n",
    "        return generated_text[start_idx:].strip()\n",
    "    else:\n",
    "        # Return original text if tokens not found\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ebbef-1376-41eb-bf40-cb1bfddc6422",
   "metadata": {},
   "source": [
    "## Load the fine-tuned and trained gpt2-medium-lora model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2b17b4-b756-4da3-ba13-a99f3cb3aa81",
   "metadata": {},
   "source": [
    "- As there are more than 10000 records in the validation dataset, the Jupyter notebook cannot handle printing all those records.\n",
    "- Hence, I have printed only 5 records of the validation dataset to make you better understand.\n",
    "- Detailed prediction will be demonstrated using the Chatbot UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab2ca242-544a-47e6-b8eb-235b96a2a3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba736a7f847c49cb9694c4382904ddd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  19%|#9        | 294M/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c36c74ccb149c1a327db85a3304276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------compare--------\n",
      "\n",
      "input:\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Question: Which NFL team represented the AFC at Super Bowl 50?\n",
      " <|start_answer|>\n",
      "\n",
      "answer:\n",
      "Cleveland Browns\n",
      "Cleveland Browns/Atlanta Falcons\n",
      "The Cleveland Browns won the Super Bowl with a 24‚Äì10 victory over the Atlanta Falcons in Super\n",
      "\n",
      "-------compare--------\n",
      "\n",
      "input:\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Question: Which NFL team represented the NFC at Super Bowl 50?\n",
      " <|start_answer|>\n",
      "\n",
      "answer:\n",
      "A: The NFL Championship Game.\n",
      "Question: Which team represented the NFC at Super Bowl 51?\n",
      "<|start_answer|>A: The NFL\n",
      "\n",
      "-------compare--------\n",
      "\n",
      "input:\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Question: Where did Super Bowl 50 take place?\n",
      " <|start_answer|>\n",
      "\n",
      "answer:\n",
      "After a day's preparation and preparation for the game, the NFL and NFL Players Association (NFLPA) agreed to a plan to play a special game on February\n",
      "\n",
      "-------compare--------\n",
      "\n",
      "input:\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Question: Which NFL team won Super Bowl 50?\n",
      " <|start_answer|>\n",
      "\n",
      "answer:\n",
      "A: The Houston Texans (2003)\n",
      "Question: Which NFL team won Super Bowl 50?\n",
      "<|start_answer|>A: The Seattle Seahawks\n",
      "\n",
      "-------compare--------\n",
      "\n",
      "input:\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Question: What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
      " <|start_answer|>\n",
      "\n",
      "answer:\n",
      "The color of the Super Bowl logo was black, which was chosen to emphasize the anniversary of the game. The color of the 50th Anniversary logo, which was\n"
     ]
    }
   ],
   "source": [
    "# load fine-tuning model\n",
    "ft_model = AutoPeftModelForCausalLM.from_pretrained(\"gpt2-medium-lora\")\n",
    "\n",
    "inputs = format_prompts(validation_ds)\n",
    "outputs = ft_model.generate(**inputs, max_new_tokens=32, do_sample=True, top_k=50, top_p=0.95, temperature=0.7, num_return_sequences=1)\n",
    "\n",
    "# generate anwsers\n",
    "generated_answers = [extract_answer(tokenizer.decode(output, skip_special_tokens=True)) for output in outputs]\n",
    "\n",
    "# display result\n",
    "for input, output, ans in zip(inputs['input_ids'], outputs, generated_answers):\n",
    "    print(\"\\n-------compare--------\")\n",
    "    print(f\"\\ninput:\\n{tokenizer.decode(input, skip_special_tokens=True)}\")\n",
    "    print(f\"\\nanswer:\\n{ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132dce10-8ca2-4a11-9639-8aeefb5bf3d6",
   "metadata": {},
   "source": [
    "## Evaluate the predictions using `evaluate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3674eb3-08a7-4d4c-9cf3-cc4dd23d1139",
   "metadata": {},
   "source": [
    "- In this code, we are assessing the degree to which our model's responses align with the accurate responses in the SQuAD dataset.\n",
    "- Initially, we import the SQuAD evaluation metric to evaluate our model's performance.\n",
    "- Next, we create two separate lists: one for the predictions made by our model and another for the actual correct answers.\n",
    "- To make predictions, we generate a list of dictionaries containing the ID of each example along with the model's generated answer.\n",
    "- In the same way, we generate a reference list with the IDs and accurate answers from the dataset.\n",
    "- In the end, we utilize the evaluation metric to analyze the predictions in comparison to the right answers, calculate the outcomes, and then display them.\n",
    "- This procedure assists us in understanding the precision of our model's responses to queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3125aebb-203d-46b5-9fcb-776c718302fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bd03b5b12b467da9ba3518d55f23a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7368023044df4dfea7cb38ffca9bbd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.0, 'f1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# evaluate based on squad metric\n",
    "ft_squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "# prepare prediciotns\n",
    "predictions = [{'id': example['id'],\n",
    "                'prediction_text': answer} for example, answer in zip(validation_ds, generated_answers)]\n",
    "\n",
    "# prepare references\n",
    "references = [{'id': example['id'],\n",
    "               'answers': example['answers']} for example in validation_ds]\n",
    "\n",
    "# compute and print result\n",
    "ft_results = ft_squad_metric.compute(predictions=predictions, references=references)\n",
    "print(ft_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
